{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b13ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:08.824581Z",
     "iopub.status.busy": "2025-01-11T17:04:08.823872Z",
     "iopub.status.idle": "2025-01-11T17:04:16.820141Z",
     "shell.execute_reply": "2025-01-11T17:04:16.819209Z"
    },
    "papermill": {
     "duration": 8.003366,
     "end_time": "2025-01-11T17:04:16.822233",
     "exception": false,
     "start_time": "2025-01-11T17:04:08.818867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import balanced_accuracy_score, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d29dd09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:16.829264Z",
     "iopub.status.busy": "2025-01-11T17:04:16.828547Z",
     "iopub.status.idle": "2025-01-11T17:04:16.832538Z",
     "shell.execute_reply": "2025-01-11T17:04:16.831748Z"
    },
    "papermill": {
     "duration": 0.009135,
     "end_time": "2025-01-11T17:04:16.834327",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.825192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tile_size= 224\n",
    "batch_size = 3\n",
    "n_tiles = 400\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120f74bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:16.840240Z",
     "iopub.status.busy": "2025-01-11T17:04:16.839985Z",
     "iopub.status.idle": "2025-01-11T17:04:16.846793Z",
     "shell.execute_reply": "2025-01-11T17:04:16.845990Z"
    },
    "papermill": {
     "duration": 0.01156,
     "end_time": "2025-01-11T17:04:16.848412",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.836852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_wsi_with_otsu(img_pil, output_path=None):\n",
    "    \"\"\"\n",
    "    Load a large image (e.g., a WSI thumbnail or a TMA),\n",
    "    generate an Otsu mask in grayscale, find the bounding box\n",
    "    of the tissue region, and crop the entire image to that box.\n",
    "\n",
    "    input_path: full path to the .png (or .tif, .jpg, etc.) image\n",
    "    output_path: if provided, save the cropped image to this path\n",
    "                 if None, just return the cropped PIL Image object\n",
    "    \"\"\"\n",
    "    # # 1) Load image in RGB (PIL -> NumPy)\n",
    "    # img_pil = Image.open(input_path).convert(\"RGB\")\n",
    "    img_rgb = np.array(img_pil)  # shape: (H, W, 3)\n",
    "\n",
    "    # 2) Convert to grayscale for Otsu\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # 3) Otsu threshold - background vs. tissue\n",
    "    \n",
    "    _, raw_mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    fraction_white = (raw_mask > 0).mean()  # fraction of pixels that are 255\n",
    "\n",
    "    if fraction_white > 0.5:\n",
    "        mask = 255 - raw_mask  # invert\n",
    "    else:\n",
    "        mask = raw_mask\n",
    "    \n",
    "    # Convert to binary {0,1}\n",
    "    mask_binary = (mask > 0).astype(np.uint8)\n",
    "    # 4) Find bounding box\n",
    "    #    We want the min/max row/col where mask is non-zero\n",
    "    coords = np.where(mask > 0)  # returns (row_indices, col_indices)\n",
    "    if len(coords[0]) == 0:\n",
    "        # No tissue found, fallback or skip\n",
    "        print(\"Warning: Otsu found no tissue; returning original image\")\n",
    "        cropped_img = img_pil\n",
    "    else:\n",
    "        y_min, y_max = coords[0].min(), coords[0].max()\n",
    "        x_min, x_max = coords[1].min(), coords[1].max()\n",
    "\n",
    "        # 5) Crop the original RGB\n",
    "        cropped_img = img_pil.crop((x_min, y_min, x_max+1, y_max+1))\n",
    "        # Note: +1 to include that pixel.\n",
    "\n",
    "    # 6) Save or return\n",
    "    # if output_path:\n",
    "    #     cropped_img.save(output_path)\n",
    "    #     print(f\"Cropped image saved to: {output_path}\")\n",
    "    return cropped_img\n",
    "\n",
    "# Example usage:\n",
    "# cropped_image = crop_wsi_with_otsu(\"slide1.png\", \"slide1_cropped.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96bc84c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:16.854465Z",
     "iopub.status.busy": "2025-01-11T17:04:16.854207Z",
     "iopub.status.idle": "2025-01-11T17:04:16.860940Z",
     "shell.execute_reply": "2025-01-11T17:04:16.860267Z"
    },
    "papermill": {
     "duration": 0.011434,
     "end_time": "2025-01-11T17:04:16.862361",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.850927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cropped_image(image, th_area=1000):\n",
    "    # Calculate the aspect ratio\n",
    "    as_ratio = image.size[0] / image.size[1]\n",
    "    \n",
    "    if as_ratio >= 1.5:\n",
    "        # Create a mask using maximum value condition\n",
    "        mask = np.max(np.array(image) > 0, axis=-1).astype(np.uint8)\n",
    "        \n",
    "        # Find connected components in the mask\n",
    "        retval, labels = cv2.connectedComponents(mask)\n",
    "        \n",
    "        for label in range(1, retval):\n",
    "            # Skip small components\n",
    "            area = np.sum(labels == label)\n",
    "            if area < th_area:\n",
    "                continue\n",
    "            \n",
    "            # Get coordinates of the first valid connected component\n",
    "            x, y = np.meshgrid(np.arange(image.size[0]), np.arange(image.size[1]))\n",
    "            xs, ys = x[labels == label], y[labels == label]\n",
    "            \n",
    "            # Calculate cropping boundaries\n",
    "            sx, ex = np.min(xs), np.max(xs)\n",
    "            cx = (sx + ex) // 2\n",
    "            crop_size = image.size[1]\n",
    "            sx = max(0, cx - crop_size // 2)\n",
    "            ex = min(sx + crop_size - 1, image.size[0] - 1)\n",
    "            sx = ex - crop_size + 1\n",
    "            sy, ey = 0, image.size[1] - 1\n",
    "            \n",
    "            # Crop the image and return\n",
    "            cropped_image = image.crop((sx, sy, ex + 1, ey + 1))\n",
    "            return cropped_image\n",
    "    else:\n",
    "        # If aspect ratio is less than 1.5, use the entire image\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecced151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:16.868587Z",
     "iopub.status.busy": "2025-01-11T17:04:16.868330Z",
     "iopub.status.idle": "2025-01-11T17:04:16.877303Z",
     "shell.execute_reply": "2025-01-11T17:04:16.876682Z"
    },
    "papermill": {
     "duration": 0.013938,
     "end_time": "2025-01-11T17:04:16.878827",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.864889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################\n",
    "# Dataset\n",
    "########################################\n",
    "class WSI_TMA_MILDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, n_tiles, tile_size, transform=None):\n",
    "        \"\"\"\n",
    "        df: DataFrame with columns ['image_id', 'label', 'is_tma']\n",
    "        data_dir: directory containing 'train_images' and 'train_thumbnails'\n",
    "        n_tiles: number of tiles per image\n",
    "        tile_size: size of each tile\n",
    "        transform: torchvision transforms\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.n_tiles = n_tiles\n",
    "        self.tile_size = tile_size\n",
    "        self.transform = transform\n",
    "        # Adjust mapping as per your classes\n",
    "        self.label_mapping = { 'HGSC':0, 'EC':1, 'CC':2, 'LGSC':3, 'MC':4 }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        label_str = row['label']\n",
    "        label = self.label_mapping[label_str]\n",
    "        is_tma = row['is_tma']\n",
    "\n",
    "        if is_tma:\n",
    "            # TMA image\n",
    "            img_path = os.path.join(self.data_dir, 'train_images', f\"{image_id}.png\")\n",
    "        else:\n",
    "            # WSI thumbnail\n",
    "            img_path = os.path.join(self.data_dir, 'train_thumbnails', f\"{image_id}_thumbnail.png\")\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        img_pil = Image.fromarray(img)\n",
    "        cropped1 = get_cropped_image(img_pil)\n",
    "        cropped2 = crop_wsi_with_otsu(cropped1)\n",
    "        img_rgb = cv2.cvtColor(np.array(cropped2), cv2.COLOR_BGR2RGB)\n",
    "        h, w, c = img_rgb.shape\n",
    "\n",
    "        tiles = []\n",
    "        # Resize the image to 4096x4096\n",
    "        img_resized = Image.fromarray(img_rgb).resize((8192, 8192))\n",
    "        img_resized = np.array(img_resized)\n",
    "\n",
    "        # Determine the size of each tile (assume 20x20 grid for 400 tiles)\n",
    "        tile_size = 8192 // 20\n",
    "\n",
    "        # Create 400 tiles of equal size\n",
    "        for i in range(20):  # Loop over rows\n",
    "            for j in range(20):  # Loop over columns\n",
    "                # Calculate the coordinates of the current tile\n",
    "                x_start = j * tile_size\n",
    "                y_start = i * tile_size\n",
    "                tile = img_resized[y_start:y_start + tile_size, x_start:x_start + tile_size, :]\n",
    "        \n",
    "                # Convert the tile to a PIL Image\n",
    "                tile_img = Image.fromarray(tile)\n",
    "        \n",
    "                # Apply transformation if specified\n",
    "                if self.transform:\n",
    "                    tile_img = self.transform(tile_img)\n",
    "        \n",
    "                tiles.append(tile_img)\n",
    "\n",
    "        tiles_tensor = torch.stack(tiles, dim=0)\n",
    "        return tiles_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba6c6c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:16.884986Z",
     "iopub.status.busy": "2025-01-11T17:04:16.884736Z",
     "iopub.status.idle": "2025-01-11T17:04:16.890974Z",
     "shell.execute_reply": "2025-01-11T17:04:16.890302Z"
    },
    "papermill": {
     "duration": 0.011152,
     "end_time": "2025-01-11T17:04:16.892515",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.881363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# MultiPatchViTExtractor\n",
    "########################################\n",
    "class MultiPatchViTExtractor:\n",
    "    \"\"\"\n",
    "    Loads two ViT models (patch8 and patch16) from timm, extracts features from each,\n",
    "    and concatenates them into a single vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        \n",
    "        # Example model names\n",
    "        self.model_patch8  = timm.create_model(\"vit_small_patch8_224\",  pretrained=False)\n",
    "        self.model_patch16 = timm.create_model(\"vit_small_patch16_224\", pretrained=False)\n",
    "\n",
    "        # load lunit weights\n",
    "        self.model_patch8.load_state_dict(\n",
    "            torch.load(\"/kaggle/input/lunit-dino-weights/dino_vit_small_patch8_ep200.torch\", \n",
    "                       map_location=\"cpu\"), strict=False)\n",
    "        self.model_patch16.load_state_dict(\n",
    "            torch.load(\"/kaggle/input/lunit-dino-weights/dino_vit_small_patch16_ep200.torch\", \n",
    "                       map_location=\"cpu\"), strict=False)\n",
    "\n",
    "        # Remove classification heads\n",
    "        self.model_patch8.head  = nn.Identity()\n",
    "        self.model_patch16.head = nn.Identity()\n",
    "        \n",
    "        self.model_patch8.to(device).eval()\n",
    "        self.model_patch16.to(device).eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, image_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects image_tensor shape: (1, 3, 224, 224).\n",
    "        Returns concatenated features from patch8 & patch16, e.g. shape: (768+768,).\n",
    "        \"\"\"\n",
    "        feats8  = self.model_patch8(image_tensor).squeeze(0)   # shape (768,)\n",
    "        feats16 = self.model_patch16(image_tensor).squeeze(0)  # shape (768,)\n",
    "        return torch.cat([feats8, feats16], dim=0)  # shape (1536,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff153f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:16.899705Z",
     "iopub.status.busy": "2025-01-11T17:04:16.899455Z",
     "iopub.status.idle": "2025-01-11T17:04:16.907618Z",
     "shell.execute_reply": "2025-01-11T17:04:16.906983Z"
    },
    "papermill": {
     "duration": 0.014019,
     "end_time": "2025-01-11T17:04:16.909115",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.895096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# MILAttentionModel (Double ViT Backbone)\n",
    "########################################\n",
    "class MILAttentionDoubleDINO(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Instance Learning model that uses the MultiPatchViTExtractor to get features \n",
    "    from each tile, applies attention, and then classifies.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=\"cpu\", num_classes=5, embed_dim=512):\n",
    "        super().__init__()\n",
    "        # Instead of inception_v3, we use our double-ViT extractor\n",
    "        self.extractor = MultiPatchViTExtractor(device=device)\n",
    "        \n",
    "        # The concatenated output of patch8 and patch16 is 1536 dims\n",
    "        in_features = 768\n",
    "\n",
    "        # Project to an embedding space if needed\n",
    "        self.embed = nn.Linear(in_features, embed_dim)\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.attention_A = nn.Linear(embed_dim, 160)\n",
    "        self.attention_B = nn.Linear(160, 1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [B, N, C, H, W]\n",
    "        B: batch size (# of patients/slides)\n",
    "        N: # of tiles per slide\n",
    "        C, H, W: channels, height, width (e.g., 3, 224, 224)\n",
    "        \"\"\"\n",
    "        B, N, C, H, W = x.shape\n",
    "        \n",
    "        # We'll accumulate all tile features in a list, then stack\n",
    "        all_features = []\n",
    "        for b_idx in range(B):\n",
    "            # Extract features for N tiles in the current batch element\n",
    "            tile_features = []\n",
    "            for n_idx in range(N):\n",
    "                # Each tile is shape [C, H, W]\n",
    "                tile = x[b_idx, n_idx, ...].unsqueeze(0).to(self.device)  # shape [1, C, H, W]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    feats = self.extractor.extract_features(tile)  # shape [1536,]\n",
    "                tile_features.append(feats.unsqueeze(0))  # shape [1, 1536]\n",
    "            \n",
    "            tile_features = torch.cat(tile_features, dim=0)  # shape [N, 1536]\n",
    "            all_features.append(tile_features.unsqueeze(0))  # shape [1, N, 1536]\n",
    "        \n",
    "        # Concatenate along batch dimension: [B, N, 1536]\n",
    "        all_features = torch.cat(all_features, dim=0).to(self.device)\n",
    "        \n",
    "        # Project to embedding dimension\n",
    "        embeddings = self.embed(all_features)  # [B, N, embed_dim]\n",
    "        \n",
    "        # Attention\n",
    "        A = torch.relu(self.attention_A(embeddings))  # [B, N, 128]\n",
    "        A = self.attention_B(A)                       # [B, N, 1]\n",
    "        A = torch.softmax(A, dim=1)                   # attention weights over tiles\n",
    "        \n",
    "        # Weighted sum of embeddings\n",
    "        weighted_sum = torch.sum(A * embeddings, dim=1)  # [B, embed_dim]\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(weighted_sum)  # [B, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a212a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:16.915169Z",
     "iopub.status.busy": "2025-01-11T17:04:16.914930Z",
     "iopub.status.idle": "2025-01-11T17:04:16.991823Z",
     "shell.execute_reply": "2025-01-11T17:04:16.990850Z"
    },
    "papermill": {
     "duration": 0.081863,
     "end_time": "2025-01-11T17:04:16.993566",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.911703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Setup\n",
    "########################################\n",
    "data_dir = \"/kaggle/input/UBC-OCEAN/\"\n",
    "df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "\n",
    "# Example label mapping already defined in dataset class\n",
    "y = df['label']\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.2,contrast=.2,saturation=.2,hue=.2),\n",
    "    transforms.Resize((224,224)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2610c5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:17.000330Z",
     "iopub.status.busy": "2025-01-11T17:04:17.000094Z",
     "iopub.status.idle": "2025-01-11T17:04:17.018452Z",
     "shell.execute_reply": "2025-01-11T17:04:17.017856Z"
    },
    "papermill": {
     "duration": 0.023549,
     "end_time": "2025-01-11T17:04:17.020068",
     "exception": false,
     "start_time": "2025-01-11T17:04:16.996519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split WSI data\n",
    "wsi_df = df[df['is_tma'] == 0]\n",
    "train_wsi, val_wsi = train_test_split(wsi_df, test_size=0.3, stratify=wsi_df['label'], random_state=42)\n",
    "\n",
    "# Split TMA data\n",
    "tma_df = df[df['is_tma'] == 1]\n",
    "train_tma, val_tma = train_test_split(tma_df, test_size=0.6, stratify=tma_df['label'], random_state=42)\n",
    "\n",
    "# Combine splits\n",
    "train_df = pd.concat([train_wsi, train_tma]).reset_index(drop=True)\n",
    "val_df = pd.concat([val_wsi, val_tma]).reset_index(drop=True)\n",
    "\n",
    "# Load data\n",
    "train_dataset = WSI_TMA_MILDataset(train_df, data_dir=data_dir, n_tiles=n_tiles, tile_size=tile_size, transform=train_transform)\n",
    "val_dataset = WSI_TMA_MILDataset(val_df, data_dir=data_dir, n_tiles=n_tiles, tile_size=tile_size, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe022ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:04:17.026697Z",
     "iopub.status.busy": "2025-01-11T17:04:17.026394Z",
     "iopub.status.idle": "2025-01-11T22:40:20.142919Z",
     "shell.execute_reply": "2025-01-11T22:40:20.141794Z"
    },
    "papermill": {
     "duration": 20163.125603,
     "end_time": "2025-01-11T22:40:20.148524",
     "exception": false,
     "start_time": "2025-01-11T17:04:17.022921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3892678498.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"/kaggle/input/lunit-dino-weights/dino_vit_small_patch8_ep200.torch\",\n",
      "/tmp/ipykernel_23/3892678498.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"/kaggle/input/lunit-dino-weights/dino_vit_small_patch16_ep200.torch\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2080321493.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(resume_best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.4766, Val Loss: 0.6272, Val Balanced Acc: 0.7289, Val AUPRC: 0.8453\n",
      "  * Best model saved.\n",
      "\n",
      "Epoch 2/6\n",
      "Epoch 2 - Train Loss: 0.4129, Val Loss: 0.7092, Val Balanced Acc: 0.6905, Val AUPRC: 0.8176\n",
      "\n",
      "Epoch 3/6\n",
      "Epoch 3 - Train Loss: 0.3936, Val Loss: 0.6624, Val Balanced Acc: 0.7361, Val AUPRC: 0.8374\n",
      "  * Best model saved.\n",
      "\n",
      "Epoch 4/6\n",
      "Epoch 4 - Train Loss: 0.3560, Val Loss: 0.8804, Val Balanced Acc: 0.6607, Val AUPRC: 0.8498\n",
      "  * Best model saved.\n",
      "\n",
      "Epoch 5/6\n",
      "Epoch 5 - Train Loss: 0.3220, Val Loss: 0.7728, Val Balanced Acc: 0.7417, Val AUPRC: 0.8135\n",
      "  * Best model saved.\n",
      "\n",
      "Epoch 6/6\n",
      "Epoch 6 - Train Loss: 0.2861, Val Loss: 0.7747, Val Balanced Acc: 0.7673, Val AUPRC: 0.8500\n",
      "  * Best model saved.\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Train and validate\n",
    "###################################\n",
    "model = MILAttentionDoubleDINO(num_classes=5, device=device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)#, weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=5e-5)\n",
    "resume_best_model_path = \"/kaggle/input/resume-after-epoch-5/pytorch/default/1/best_model_224_400tiles_tuned (2).pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(resume_best_model_path))\n",
    "\n",
    "best_balanced_acc = 0.0\n",
    "best_auprc = 0.0\n",
    "patience = 4\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"best_model_224_400tiles_tuned.pth\"\n",
    "\n",
    "for epoch in range(6):  # Up to 20 epochs, adjust as needed\n",
    "    print(f\"\\nEpoch {epoch+1}/{6}\")\n",
    "    # print(f\"Current Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for tiles, labels in train_loader:\n",
    "        tiles, labels = tiles.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(tiles)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * tiles.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []  # To store probabilities for AUPRC calculation\n",
    "    with torch.no_grad():\n",
    "        for tiles, labels in val_loader:\n",
    "            tiles, labels = tiles.to(device), labels.to(device)\n",
    "            logits = model(tiles)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss_val += loss.item() * tiles.size(0)\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()  # Get probabilities\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_probs.append(probs)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "\n",
    "    val_loss = running_loss_val / len(val_loader.dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)  # Shape: (num_samples, num_classes)\n",
    "\n",
    "    # Calculate metrics\n",
    "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    # Calculate AUPRC (macro-average across all classes)\n",
    "    auprc = average_precision_score(\n",
    "        np.eye(len(all_probs[0]))[all_labels],  # One-hot encode labels\n",
    "        all_probs,\n",
    "        average=\"macro\"\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Balanced Acc: {balanced_acc:.4f}, Val AUPRC: {auprc:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if balanced_acc > best_balanced_acc or auprc > best_auprc:\n",
    "        if balanced_acc > best_balanced_acc:\n",
    "            best_balanced_acc = balanced_acc\n",
    "        if auprc > best_auprc:\n",
    "            best_auprc = auprc\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"  * Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "    # Step the scheduler\n",
    "    # scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0210843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T22:40:20.156748Z",
     "iopub.status.busy": "2025-01-11T22:40:20.156437Z",
     "iopub.status.idle": "2025-01-11T22:57:43.569299Z",
     "shell.execute_reply": "2025-01-11T22:57:43.567692Z"
    },
    "papermill": {
     "duration": 1043.420325,
     "end_time": "2025-01-11T22:57:43.572146",
     "exception": false,
     "start_time": "2025-01-11T22:40:20.151821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/244667915.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate final performance on validation set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for tiles, labels in val_loader:\n",
    "        tiles, labels = tiles.to(device), labels.to(device)\n",
    "        logits = model(tiles)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_probs = np.concatenate(all_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f7a81a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T22:57:43.581467Z",
     "iopub.status.busy": "2025-01-11T22:57:43.581153Z",
     "iopub.status.idle": "2025-01-11T22:57:43.594037Z",
     "shell.execute_reply": "2025-01-11T22:57:43.593089Z"
    },
    "papermill": {
     "duration": 0.01934,
     "end_time": "2025-01-11T22:57:43.595767",
     "exception": false,
     "start_time": "2025-01-11T22:57:43.576427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Balanced Accuracy: 0.7673\n",
      "Final AUPRC: 0.8500\n"
     ]
    }
   ],
   "source": [
    "# Final metrics\n",
    "final_balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "final_auprc = average_precision_score(\n",
    "    np.eye(len(all_probs[0]))[all_labels],\n",
    "    all_probs,\n",
    "    average=\"macro\"\n",
    ")\n",
    "print(f\"Final Balanced Accuracy: {final_balanced_acc:.4f}\")\n",
    "print(f\"Final AUPRC: {final_auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b588117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T22:57:43.604152Z",
     "iopub.status.busy": "2025-01-11T22:57:43.603469Z",
     "iopub.status.idle": "2025-01-11T22:57:43.609102Z",
     "shell.execute_reply": "2025-01-11T22:57:43.608245Z"
    },
    "papermill": {
     "duration": 0.012311,
     "end_time": "2025-01-11T22:57:43.611527",
     "exception": false,
     "start_time": "2025-01-11T22:57:43.599216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully: all_labels.npy, all_probs.npy\n"
     ]
    }
   ],
   "source": [
    "np.save('/kaggle/working/all_labels.npy', all_labels)\n",
    "np.save('/kaggle/working/all_probs.npy', all_probs)\n",
    "\n",
    "print(\"Files saved successfully: all_labels.npy, all_probs.npy\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6924515,
     "sourceId": 45867,
     "sourceType": "competition"
    },
    {
     "datasetId": 4107067,
     "sourceId": 7120688,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 215152,
     "modelInstanceId": 193210,
     "sourceId": 226547,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21219.951329,
   "end_time": "2025-01-11T22:57:46.364290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-11T17:04:06.412961",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
